\documentclass{article} % For LaTeX2e
\usepackage{enumitem,amssymb,latexsym,amsmath,graphicx,amsthm,xkeyval,subfig,siunitx,booktabs}
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\title{CS230 Stock Price Prediction}

\author{
Ryan Almodovar \\
Stanford University\\
\texttt{ralmodov@stanford.edu} \\
\And
Vivek Misra \\
Stanford University \\
\texttt{vmisra@stanford.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{\textbf{TODO: {#1}}}
\newcommand\fnurl[2]{%
\href{#2}{#1}\footnote{\url{#2}}%
}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

% TODO
% \begin{abstract}
% \end{abstract}

\section{Introduction}


\section{Related Work}

The model which our improvements are built upon are from the CS224N Winter 2018 SQuAD final project 
on \fnurl{GitHub}{https://github.com/abisee/cs224n-win18-squad}.
The improvements are mainly based from the Coattention Encoder architecture introduced
from the DCN model, which replaces the existing basic attention layer.
The contextual embedding layers for the question and document that have been included in the base project
are based on a simplified version of the Bidirectional Attention Flow (Seo et al., 2017) architecture,
with the exclusion of the character CNN embeddings (Kim et al., 2016),
and the substitution of the main attention layer with a basic attention layer.
The recently released Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016),
provides over 100,000 question/document pairs and answers, allowing for a variety of qualities that
culminate in a natural QA task.
The performance based on the SQuAD dataset of the comprehension system is then uploaded
and evaluated on \fnurl{CodaLab}{http://codalab.org}.

\pagebreak

\section{Architecture and Approach}
\label{architecture}
\begin{center}%
\begin{figure}[hp!]%
\hspace*{-1cm}%
\includegraphics[width=\dimexpr\textwidth+2cm\relax]{img/architecture-overview}
\caption{Architecture Overview}
\hspace*{-1cm}%
\end{figure}
\end{center}%

\subsection{Contextual Embedding Layer}

The GRU cells (Chung et al., 2014) in the existing model for the bidirectional RNN to form the encodings
for question and document word embeddings are replaced with LSTM (Hochreiter et al., 1997)
encoder word sequences to slightly increase the Dev EM and F1 scores from the baseline and
Coattention models as seen in Table \ref{tab:scores}, and are defined as
$X^Q = [x_1^Q , x_2^Q, ..., x_n^Q]$ and $X^D = [x_1^D , x_2^D, ..., x_m^D]$ respectively.

\subsection{Coattention Layer}

The next procedure after generating the contextual embeddings is to apply the attention
layer between the two question and document embedding tensors.
The following layer replaces the existing \texttt{BasicAttn} class from the base project
with a new \texttt{CoAttn} class.
The Coattention mechanism that attends to the question and document simultaneously
is based on the Coattention encoder from the DCN model (Xiong et al., 2017),
and fuses both attention question and document contexts.
Figure \ref{fig:coattn} provides a visualization of the Coattention encoder.


\subsubsection{Question and Document Encoder Sentinels}

First, sentinel vectors $x_\varnothing^Q$ and $x_\varnothing^D$ (Merity et al., 2016)
are concatenated to each embedding respectively,
in order to allow the model to not attend to any particular word in the input
as demonstrated by the illustration in Figure \ref{fig:pointer}.

\begin{center}%
\begin{figure}[h!]%
\hspace*{-1cm}%
\includegraphics[width=\dimexpr\textwidth+2cm\relax]{img/pointer-sentinel}
\caption{Visualization of the pointer sentinel-RNN mixture model. The query is used by the pointer network to identify likely matching words from the past. Probability mass can be directed to the RNN by increasing the value of the mixture gate g via the sentinel, seen in grey.}
\label{fig:pointer}
\hspace*{-1cm}%
\end{figure}
\end{center}%


Let the resulting sequences be defined as the third-order tensors:
\begin{align}
  Q' = [x_1^Q , x_2^Q, ..., x_n^Q, x_\varnothing^Q] = [X^Q;x_\varnothing^Q] \in \mathbb{R}^{\beta\times \ell\times (n+1)} \\
  D  = [x_1^D , x_2^D, ..., x_m^D, x_\varnothing^D] = [X^D;x_\varnothing^D] \in \mathbb{R}^{\beta\times \ell\times (m+1)} 
\end{align}
where $\beta$ represents the dynamic batch size of the current iteration,
and $\ell$ represents the hidden layer size.

Next, a non-linear projection layer is then applied on top of the question encoding to allow
for variation between the question and the document encoding spaces,
as defined by:
\begin{align}
  Q = \text{tanh}(W_{ij}^{(Q)}Q_{ajk}' + b^{(Q)}) \in \mathbb{R}^{\beta\times \ell\times (n+1)}
\end{align}
where $W^{(Q)} \in \mathbb{R}^{\ell\times \ell}$ is a trainable weight matrix with
Xavier initialization (Glorot et al., 2010) and $b^{(Q)} \in \mathbb{R}^{n+1}$ is a trainable
bias vector with values initialized at zero.
The Einstein summation convention is used to convey the product result
of the matrix $W^{(Q)}$ and third-order tensor $Q'$.
 

\begin{center}%
\begin{figure}[h!]%
\hspace*{-1cm}%
\includegraphics[width=\dimexpr\textwidth+2cm\relax]{img/Coattention-layer}
\caption{Coattention encoder.
The affinity matrix L is not shown here, but 
instead directly shows the normalized attention weights $A^D$ and $A^Q$.
}
\label{fig:coattn}
\hspace*{-1cm}%
\end{figure}
\end{center}%

\subsubsection{Coattention Encoder}

To obtain affinity scores which correspond to all pairs
of the question and document words, let the affinity tensor $L$ be defined as:
\begin{align}
  L = D^TQ \in \mathbb{R}^{\beta\times (m+1)\times (n+1)}
\end{align}

Next, the affinity tensor is normalized with probability distribution with respect to
the question dimension row-wise to obtain attention weights $A^Q$,
and context dimension column-wise to obtain attention weights $A^D$ via the softmax function:
\begin{align}
  A^Q = \text{softmax}(L) \in \mathbb{R}^{\beta\times (m+1)\times (n+1)} \\
  A^D = \text{softmax}(L^T) \in \mathbb{R}^{\beta\times (n+1)\times (m+1)} \\
\end{align}

The summaries, which are essentially attention contexts of each word of the question
and the document respectively, are defined by:
\begin{align}
  C^Q = DA^Q \in \mathbb{R}^{\beta\times \ell\times (n+1)} \\
  C^D = [Q;C^Q]A^D \in \mathbb{R}^{\beta\times 2\ell\times (m+1)} \\
\end{align}
where the notation $[a;b]$ is defined as concatenation of tensors with respect to the $\ell$ dimension,
and $C^QA^D$ can be interpreted as the mapping of question encoding into space of document encodings.

Finally, a bidirectional LSTM fuses the temporal information
to the Coattention context, and the sentinel vector $x_\varnothing^D$ is
truncated, producing the attention output tensor $U$:
\begin{align}
  u_t = \text{Bi-LSTM}(u_{t-1},u_{t+1},[d_t;c_t^D]) \in \mathbb{R}^{\beta\times 2\ell} \\
  U = [u_1,...,u_m] \in \mathbb{R}^{\beta\times 2\ell\times m} \\
\end{align}
where $u_{t-1}$ and $u_{t+1}$ are the forward and backward timesteps.

The resulting tensor $U$ is then transposed so $U^T \in \mathbb{R}^{\beta\times m \times 2\ell}$
is compatible with the existing architecture to apply the matrix for masked context
values $M^D \in \mathbb{R}^{\beta\times m}$, which is essentially a binary mask with a value of 1
when there is a real value, and a 0 for padding.
Applying one last fully connected layer, which also reduces the vector space dimensionality of the $\ell$ dimension:
\begin{align}
  U' = \text{ReLU}(U^T) \in \mathbb{R}^{\beta\times m \times \ell}
\end{align}
which $U'$ is then used as the input to the layer of computing the start and end probability distributions via
a masked softmax function applied to the blended representation $[X^Q;U']$.


\pagebreak

\section{Experiments}

\subsection{Implementation and Metrics}
The model is trained and evaluated on the SQuAD dataset, and uses
pretrained GloVe word vectors on the Common Crawl corpus (Pennington et al., 2014).
Running the experiments and viewing the results on TensorBoard displayed key metrics
to determine optimal values for various hyperparameters.
Each experiment was ran for approximately 10k to 15k iterations and generally stopped
when the Dev F1 and EM scores continued to stay plateaued after several thousand iterations,
indicating that the model was not improving despite the Training F1 and EM scores rising since by that time
the model overfitting to the data was occurring.

\begin{figure}[hp!]%
\centering
\subfloat[Dev F1 Score]{{\includegraphics[width=6cm]{img/dev-f1} }}%
\qquad
\subfloat[QAModel Loss]{{\includegraphics[width=6cm]{img/qa-model-loss} }}%
\caption{
Dev F1 and QAModel Loss on TensorBoard after 12,000 iterations.
}%
\label{fig:f1_loss}%
\end{figure}

\subsection{Hyperparameters}
Based on the word length histograms of the Train and Dev datasets illustrated in Figure \ref{fig:wordlen},
the context word length from both sets were sparse past ${\sim}450$, so the hyperparameter
\texttt{context\_len} was reduced from 600 to 450 to optimize the run time for each training iteration.
To further fit the model to allow higher complexity, the hyperparameter for the GloVe word embeddings
\texttt{embedding\_size} was increased from 100 to 200, which lead to more than a 10\% F1 score increase.
We use a dropout rate (Srivastava et al., 2014) of 0.2 to regularize our network during
training to mitigate overfitting,
and optimize the model using the ADAM Optimizer (Kingma et al., 2014).

\begin{figure}[hp!]%
\centering
\subfloat[Context Word Length Frequency]{{\includegraphics[width=6cm]{img/context-len} }}%
\qquad
\subfloat[Question Word Length Frequency]{{\includegraphics[width=6cm]{img/question-len} }}%
\caption{Context and question word lengths of the provided Train and Dev datasets.
Since the Train dataset is orders of magnitude larger than the Dev dataset,
the frequency along the y-axis is normalized to visually compare the word lengths
along the x-axis more accurately.
}%
\label{fig:wordlen}%
\end{figure}

\subsection{Results}
The two evaluation metrics on the SQuAD dataset consists of exact match (EM) score, and F1 score.
The EM score is evaluated based on the exact word sequence match between the predicted answer
and a ground truth answer.
The F1 score is evaluated based on the overlap between words in the predicted answer and a ground truth answer.
Since there are some instances in which a document-question pair may have many ground truth answers,
the EM and F1 scores for a document-question pair is evaluated as the maximum value across all ground truth answers.
The Dev EM and F1 score results were recorded for each of the variations of the model
by either running the project in \texttt{official\_eval} mode
or the result if the model was uploaded to the dev set on CodaLab,
and are listed in Table \ref{tab:scores}.
Finally, the overall metric is then averaged over all document-question pairs.
The official SQuAD evaluation is hosted on CodaLab, which contains the training and development sets
that are publicly available while the secret test set is withheld.

\begin{table}[hp!]
\centering
\begin{tabular}{ll|ll}
\toprule
Model & with variation & Dev/\textit{Test} EM & Dev/\textit{Test} F1 \\
\midrule
Baseline & \textit{default} & \textbf{34.90} & \textbf{43.93} \\
 & LSTM & \textbf{35.79} & \textbf{44.95} \\
\midrule
Coattention Layer & *LSTM & 49.87 & 64.99 \\
 & lower learning rate (0.0005) & 49.08 & 63.50 \\
 & *higher dropout rate (0.2) & 50.2 & 64.90 \\
 & *200D GloVe & \textit{\textbf{61.47}} & \textit{\textbf{71.68}} \\
\midrule
\textit{References} &&& \\
DCN (Xiong et al., 2017) & Ensemble & 71.6 & 80.4 \\
BiDAF (Seo et al., 2017) & Ensemble & 73.3 & 81.1 \\
\bottomrule
\end{tabular}
\caption{
The variations marked with an asterisk* are included into the final model.
The \textbf{bolded} values are the scores received after uploading to the CodaLab Dev set.
The \textit{italicized} values are the scores received after uploading to the CodaLab Test set.
}
\label{tab:scores}%
\end{table}

\subsection{Analysis of Examples}
Due to above limitations of our implementations and also dataset, we did observe certain adversarial examples along with good ones.Some of them are listed here:
\emph{\emph{Example-1 `` How"  \textbf{•}}}

\emph{Paragraph}:``  quickbooks sponsored a `` small business big game " contest , in which death wish coffee had a 30-second commercial aired free of charge courtesy of quickbooks . death wish coffee beat out nine other contenders from across the united states for the free advertisement .'' 

\emph{Question}: how many other contestants did the company , that had their ad shown for free ,beat out?

\emph{True Answer}: nine   

 \textit{Model predicts}: nine

\emph{F1 Score }: 1.000  

\emph{EM Score}: True

\emph{Example-2 ``  How ''  \textbf{•}}

\emph{Paragraph}:``  the crew of apollo 8 sent the first live televised pictures of the earth and the moon back to earth , and read from the creation story in the book of genesis ,on christmas eve , 1968,which had been a troubled year for the us , marked by vietnam war protests , race riots , and the assassinations of civil rights leader martin luther king , jr. , and senator robert f. kennedy . '' 
\emph{Question}: how many other contestants did the company , that had their ad shown for free , beat out ?

\emph{True Answer}:one-quarter 

\textit{Model predicts}: one-quarter

\emph{F1 Score}: 1.000  

\emph{EM Score}: True

\emph{Example-3 ``  What"   \textbf{•}}

\emph{Paragraph}:``  in early 2012 , nfl commissioner roger goodell stated that the league planned to make the 50th super bowl ``  spectacular " and that it would be " an important game for us as a league  ."
\emph{Question}: what one word did the nfl commissioner use to describe what super bowl 50 was intended to be ?

\emph{True Answer}:spectacular

 \textit{Model predicts}: spectacular

\emph{F1 Score}: 1.000 

 \emph{EM Score}: True



\emph{Example-3 `` What"   \textbf{•}}

\emph{Paragraph}:``  southern california is home to many major business districts . central business districts ( cbd ) include downtown los angeles , downtown san diego , downtown san bernardino , downtown bakersfield , southcoast metro and downtown riverside ."  
\emph{Question}: what is the only district in the cbd to not have ``  downtown "   in it 's name ?

\emph{True Answer}: south coast metro

\textit{Model predicts}: central business districts

\emph{F1 Score}: 0.000 

 \emph{EM Score}: False
 
So we see above model has limitations. This may be due to contextual neighbor  support which is limitation of GloVe.

\emph{Example-4 ``  Which``   \textbf{•}}

\emph{Paragraph}:``  prime numbers have influenced many artists and writers . the french composer olivier messiaen "used prime  to create unpredictable rhythms : the primes 41 , 43 , 47 and 53 .appear in the third e\'étude ,``  neumes rythmiques " .according to messiaen this way of composing was " inspired by the movements of nature, movements of free and unequal durations `  

\emph{Question}: in which etude of neumes rythmiques do the primes 41 , 43 , 47 and 53 appear in ? 

\emph{True Answer}:the third e\'étude
 
\textit{Model predicts}:  third

\emph{F1 Score}: 0.667  
\emph{EM Score}: False
This is same issue as in Example-3
Above observation suggests that if we have pre trained senetence level  embedding it would probably solve contextual issues and accuracy can be improved.


\section{Conclusion}
We successfully implemented an end-to-end deep learning model for the "CS 224N Default Final Project: Question Answering".
We were able to achieve promising preliminary results for this very challenging problem. Our accuracy and loss curve looks  promising though further improvements are possible.
As an outcome of  this exercise, we wish to highlight certain improvements we wanted to make, certain issues which we faced and tuning which we did to achieve results.

\subsection{Further Improvements}
We definitely could have improved it by using character CNN embeddings  but due to time constraints we did not fully implement it.
With more time, we would love to investigate more hyper-parameter decisions. With more time, we would like to run a proper hyperparameter search algorithm over other parameters
(e.g. batch size, LSTM hidden layer dimension, and hopefully converge on values that would boost our performance.

Other improvements that could have been completed but were not included are
ensembling multiple models,
and implementing the Highway Maxout Network (Xiong et al., 2017)
as an intermediate layer between the Coattention layer and the output probability distribution layer.

We observed certain limitations of the SQuAD dataset which could have actually underplays the efficacy of our model. Since every answer to a question in SQuAD is a fixed pair of indices, the question answering task leaves no room for nuance and can mark other technically correct answers as incorrect.In short, our performance on the Question Answering task appears to be a good start, but our model certainly has limitations as described above along with the fact that there seems to be a theoretical
limit on just how useful SQuAD can be as a proxy for measuring reading comprehension given the lack of nuance in answer choices.

\subsection{Acknowledgments}
We would like to thank Professor Richard Socher for teaching the various models and techniques
present in the state-of-the-art publications of Natural Language Processing to allow for this
project to be accessible,
and the TAs of CS224N who consistently helped clarify core concepts from past assignments
and questions from Piazza posts which were key to the implementation of this project.


\begin{thebibliography}{1}

\small{

\bibitem{notes} Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic Coattention Networks for Question Answering
 {\em arXiv preprint arXiv:1611.01604}. 2016.

\bibitem{notes}Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hananneh Hajishirzi. Bi-Directional Attention Flow for Machine Comprehension
 {\em arXiv preprint arXiv:1611.01603}. 2017.

\bibitem{notes}Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global Vectors for Word Representation
 {\em In EMNLP, volume 14, pp. 1532–43}. 2014.

\bibitem{notes}Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture Models
 {\em arXiv preprint arXiv:1609.07843}. 2016.

\bibitem{notes}Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. Character-Aware Neural Language Models
 {\em arXiv preprint arXiv:1508.06615}. 2015.

\bibitem{notes}Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text
 {\em arXiv preprint arXiv:1606.05250}. 2016.

\bibitem{notes}Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio. 
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
 {\em arXiv preprint arXiv:1412.3555}. 2014.

\bibitem{notes}Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. 
 {\em Neural Computation}. 1997.

\bibitem{notes}Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. 
 {\em PMLR 9:249-256}. 2010.

\bibitem{notes}Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A Simple Way To prevent Neural Networks From Overfitting.
 {\em JMLR}. 2014.

\bibitem{notes}Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
 {\em arXiv preprint arXiv:1412.6980}. 2014.

}

\end{thebibliography}






\end{document}
