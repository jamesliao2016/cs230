\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Architecture and Approach}{2}{section.3}}
\newlabel{architecture}{{3}{2}{Architecture and Approach}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture Overview\relax }}{2}{figure.caption.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Contextual Embedding Layer}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Coattention Layer}{2}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Question and Document Encoder Sentinels}{2}{subsubsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of the pointer sentinel-RNN mixture model. The query is used by the pointer network to identify likely matching words from the past. Probability mass can be directed to the RNN by increasing the value of the mixture gate g via the sentinel, seen in grey.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pointer}{{2}{2}{Visualization of the pointer sentinel-RNN mixture model. The query is used by the pointer network to identify likely matching words from the past. Probability mass can be directed to the RNN by increasing the value of the mixture gate g via the sentinel, seen in grey.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Coattention encoder. The affinity matrix L is not shown here, but instead directly shows the normalized attention weights $A^D$ and $A^Q$. \relax }}{3}{figure.caption.3}}
\newlabel{fig:coattn}{{3}{3}{Coattention encoder. The affinity matrix L is not shown here, but instead directly shows the normalized attention weights $A^D$ and $A^Q$. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Coattention Encoder}{3}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation and Metrics}{4}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Dev F1 and QAModel Loss on TensorBoard after 12,000 iterations. \relax }}{4}{figure.caption.4}}
\newlabel{fig:f1_loss}{{4}{4}{Dev F1 and QAModel Loss on TensorBoard after 12,000 iterations. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Dev F1 Score}}}{4}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {QAModel Loss}}}{4}{subfigure.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Hyperparameters}{4}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Context and question word lengths of the provided Train and Dev datasets. Since the Train dataset is orders of magnitude larger than the Dev dataset, the frequency along the y-axis is normalized to visually compare the word lengths along the x-axis more accurately. \relax }}{4}{figure.caption.5}}
\newlabel{fig:wordlen}{{5}{4}{Context and question word lengths of the provided Train and Dev datasets. Since the Train dataset is orders of magnitude larger than the Dev dataset, the frequency along the y-axis is normalized to visually compare the word lengths along the x-axis more accurately. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Context Word Length Frequency}}}{4}{subfigure.5.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Question Word Length Frequency}}}{4}{subfigure.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{4}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Analysis of Examples}{5}{subsection.4.4}}
\bibcite{notes}{1}
\bibcite{notes}{2}
\bibcite{notes}{3}
\bibcite{notes}{4}
\bibcite{notes}{5}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Further Improvements}{6}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Acknowledgments}{6}{subsection.5.2}}
\bibcite{notes}{6}
\bibcite{notes}{7}
\bibcite{notes}{8}
\bibcite{notes}{9}
\bibcite{notes}{10}
\bibcite{notes}{11}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  The variations marked with an asterisk* are included into the final model. The \textbf  {bolded} values are the scores received after uploading to the CodaLab Dev set. The \textit  {italicized} values are the scores received after uploading to the CodaLab Test set. \relax }}{8}{table.caption.6}}
\newlabel{tab:scores}{{1}{8}{The variations marked with an asterisk* are included into the final model. The \textbf {bolded} values are the scores received after uploading to the CodaLab Dev set. The \textit {italicized} values are the scores received after uploading to the CodaLab Test set. \relax }{table.caption.6}{}}
